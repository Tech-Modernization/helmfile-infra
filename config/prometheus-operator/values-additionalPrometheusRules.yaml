additionalPrometheusRules:
  - name: kubeapi
    groups:
      - name: kubeapi_alerts
        rules:
        - alert: KubeAPIUnHealthy
          annotations:
            description: 'Issue: Kube API is not responding 200s from blackbox.monitoring
      
              Playbook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIUnHealthy
      
              '
            summary: Kube API is unhealthy
          expr: probe_success{provider="kubernetes"} == 0
          for: 5m
          labels:
            notify_to: slack
            severity: critical
            slack_channel: '#sre-alerts'
        - alert: KubeAPIErrorRatioHigh
          annotations:
            description: 'Issue: Kube API Error ratio on {{ $labels.instance }} is above
              0.01: {{ $value }}
      
              Playbook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIErrorRatioHigh
      
              '
            summary: Kube API 500s ratio is High
          expr: sum by (instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb=~"GET|POST|DELETE|PATCH",
            code=~"5.."}) > 0.01
          for: 5m
          labels:
            notify_to: slack
            severity: critical
            slack_channel: '#sre-alerts'
        - alert: KubeAPILatencyHigh
          annotations:
            description: 'Issue: Kube API Latency on {{ $labels.instance }} is above 200
              ms: {{ $value }}
      
              Playbook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPILatencyHigh
      
              '
            summary: Kube API Latency is High
          expr: max by (instance)(kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m{verb=~"GET|POST|DELETE|PATCH"})
            > 200
          for: 5m
          labels:
            notify_to: slack
            severity: critical
            slack_channel: '#sre-alerts'
        - alert: KubeControllerWorkDurationHigh
          annotations:
            description: 'Issue: Kube Control Manager on {{ $labels.instance }} work duration
              is above 100: {{ $value }}
      
              Playbook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeControllerWorkDurationHigh
      
              '
            summary: Kube Control Manager workqueue processing is slow
          expr: sum by (instance)( APIServiceRegistrationController_work_duration{quantile="0.9"}
            ) > 100
          for: 5m
          labels:
            notify_to: slack
            severity: critical
            slack_channel: '#sre-alerts'
        - alert: KubeEtcdLatencyHigh
          annotations:
            description: 'Issue: Kube Etcd latency on {{ $labels.instance }} above 2000
              ms: {{ $value }}
      
              Playbook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeEtcdLatencyHigh
      
              '
            summary: Etcd Latency is High
          expr: max by (instance)( etcd_request_latencies_summary{job="kubernetes_apiservers",quantile="0.9"}
            )/ 1e3 > 2000
          for: 5m
          labels:
            notify_to: slack
            severity: critical
            slack_channel: '#sre-alerts'

      - name: kubeapi_rules
        rules:
        - expr: histogram_quantile ( 0.90, sum by (le, verb)( rate(apiserver_request_latencies_bucket[5m])
            ) ) / 1e3 > 0
          labels:
            job: kubernetes_api_slo
          record: kubernetes:job_verb:apiserver_latency:pctl90rate5m
        - expr: histogram_quantile ( 0.90, sum by (le, job, verb, instance)( rate(apiserver_request_latencies_bucket[5m])
            ) ) / 1e3
          labels:
            job: kubernetes_api_slo
          record: kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m
        - expr: sum by()(probe_success{provider="kubernetes", component="apiserver"})
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job:probe_success
        - expr: sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job_verb_code:apiserver_requests:rate5m
        - expr: sum by (job, verb, code, instance)(rate(apiserver_request_count[5m]))
          labels:
            job: kubernetes_api_slo
          record: kubernetes:job_verb_code_instance:apiserver_requests:rate5m
        - expr: sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m)
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job_verb_code:apiserver_requests:ratio_rate5m
        - expr: kubernetes:job_verb_code_instance:apiserver_requests:rate5m / ignoring(verb,
            code) group_left sum by (job, instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)
          labels:
            job: kubernetes_api_slo
          record: kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m
        - expr: sum by (job)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb=~"GET|POST|DELETE|PATCH",
            code=~"5.."})
          labels:
            job: kubernetes_api_slo
          record: kubernetes:job:apiserver_request_errors:ratio_rate5m
        - expr: histogram_quantile ( 0.90, sum by (le, job)( rate(apiserver_request_latencies_bucket{verb=~"GET|POST|DELETE|PATCH"}[5m])
            ) ) / 1e3
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job:apiserver_latency:pctl90rate5m
        - expr: kubernetes:job:apiserver_request_errors:ratio_rate5m < bool 0.01 * kubernetes::job:apiserver_latency:pctl90rate5m
            < bool 200
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job:slo_kube_api_ok
        - expr: kubernetes:job:apiserver_request_errors:ratio_rate5m < bool Inf * kubernetes::job:apiserver_latency:pctl90rate5m
            < bool Inf
          labels:
            job: kubernetes_api_slo
          record: kubernetes::job:slo_kube_api_sample

